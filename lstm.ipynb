{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "import spacy\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pytorch version:  1.1.0\n",
      "Currently selected device:  0\n",
      "# GPUs available:  2\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "print('Pytorch version: ', torch.__version__)\n",
    "print('Currently selected device: ', torch.cuda.current_device())\n",
    "print('# GPUs available: ', torch.cuda.device_count())\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = torch.device('cpu') # デバッグ用\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('/home/b2018yniki/data/semeval2010task8/train_original.tsv', sep='\\t')\n",
    "train_df = train_df.assign(causal_flag = [1 if 'Cause-Effect' in relation else 0 for relation in train_df.relation.values]).drop(['relation', 'comment'], axis=1)\n",
    "train_df.body = [text.replace('\"', '') for text in train_df.body.values]\n",
    "test_df = pd.read_csv('/home/b2018yniki/data/semeval2010task8/test_original.tsv', sep='\\t')\n",
    "test_df = test_df.assign(causal_flag = [1 if 'Cause-Effect' in relation else 0 for relation in test_df.relation.values]).drop(['relation', 'comment'], axis=1)\n",
    "test_df.body = [text.replace('\"', '') for text in test_df.body.values]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_en = spacy.load('en')\n",
    "def tokenizer(text):\n",
    "    soup = BeautifulSoup(text)\n",
    "    clean_txt = soup.get_text()\n",
    "    words = []\n",
    "    for tok in spacy_en.tokenizer(clean_txt):\n",
    "        if tok.text not in \"[],.();:<>{}|*-~\":\n",
    "            words.append(tok.text)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulaly size: 21100\n"
     ]
    }
   ],
   "source": [
    "vocab = []\n",
    "for text in train_df.body.values:\n",
    "    vocab.extend(tokenizer(text))\n",
    "vocab = list(set(vocab))\n",
    "print('vocabulaly size: {}'.format(len(vocab)))\n",
    "vocab_idx = dict(zip(vocab, range(len(vocab))))\n",
    "del vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df2input(df, vocab_idx):\n",
    "    data = []\n",
    "    for text in df.body.values:\n",
    "        words = tokenizer(text)\n",
    "        data.append([vocab_idx[word] for word in words if word in vocab_idx.keys()])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data, tags):\n",
    "        super(MyDataset, self).__init__()\n",
    "        assert len(data) == len(tags)\n",
    "        # npに変換し、0埋めを行う\n",
    "        max_length = max([len(d) for d in data])\n",
    "        self.data = np.zeros((len(tags), max_length))\n",
    "        for i, d1 in enumerate(data):\n",
    "            for l, d2 in enumerate(d1):\n",
    "                self.data[i][l] = d2\n",
    "        self.tags = tags\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tags)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index], self.tags[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df2input(train_df, vocab_idx)\n",
    "X_test  = df2input(test_df, vocab_idx)\n",
    "\n",
    "train_ds = MyDataset(X_train, train_df.causal_flag)\n",
    "test_ds  = MyDataset(X_test, test_df.causal_flag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, batch_size, vocab_size, emb_dim, hidden_dim, dropout_rate=0.0, activate='tanh', bidirectional=False):\n",
    "        super(LSTM, self).__init__()\n",
    "        \n",
    "        self.vocab_size = vocab_size\n",
    "        self.emb_dim    = emb_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.batch_size = batch_size\n",
    "        self.bidirectional = bidirectional\n",
    "        self.activate   = activate\n",
    "        \n",
    "        self.emb  = nn.Embedding(self.vocab_size, self.emb_dim)\n",
    "        self.lstm = nn.LSTM(self.emb_dim, self.hidden_dim, batch_first=True, bidirectional=self.bidirectional)\n",
    "        \n",
    "        self.fc0 = nn.Linear(hidden_dim * 2, 100)\n",
    "        self.fc1 = nn.Linear(100, 2)\n",
    "        self.do  = nn.Dropout(dropout_rate)\n",
    "        \n",
    "        self.hidden = self.init_hidden()\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.emb(x)\n",
    "        lstm_out, self.hidden = self.lstm(x, self.hidden)\n",
    "        y = self.fc0(torch.cat([self.hidden[0][-1], self.hidden[0][-2]], 1))\n",
    "        y = self.do(y)\n",
    "        if self.activate == 'tanh':\n",
    "            y = self.fc1(F.tanh(y))\n",
    "        elif self.activate == 'relu':\n",
    "            y = self.fc1(F.relu(y))\n",
    "        tag_scores = F.log_softmax(y)\n",
    "        return tag_scores\n",
    "\n",
    "    def init_hidden(self):\n",
    "        # The axes semantics are (num_layers, minibatch_size, hidden_dim)\n",
    "        num = 2 if self.bidirectional else 1    # bidirectionalのとき2\n",
    "        h0 = torch.zeros(num, self.batch_size, self.hidden_dim).to(device)\n",
    "        c0 = torch.zeros(num, self.batch_size, self.hidden_dim).to(device)\n",
    "        return (h0, c0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot loss and acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_plot(epoch_num, train_loss_list, train_acc_list, valid_loss_list, valid_acc_list):\n",
    "    plt.figure()\n",
    "    plt.plot(range(epoch_num), train_loss_list, color='blue', linestyle='-', label='train_loss')\n",
    "    plt.plot(range(epoch_num), valid_loss_list, color='green', linestyle='--', label='val_loss')\n",
    "    plt.legend()\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('loss')\n",
    "    plt.title('Training and validation loss')\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(range(epoch_num), train_acc_list, color='blue', linestyle='-', label='train_acc')\n",
    "    plt.plot(range(epoch_num), valid_acc_list, color='green', linestyle='--', label='val_acc')\n",
    "    plt.legend()\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('acc')\n",
    "    plt.title('Training and validation accuracy')\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(net, train_loader, valid_loader, epoch_num, y_valid):\n",
    "    train_loss_list = []\n",
    "    train_acc_list  = []\n",
    "    valid_loss_list = []\n",
    "    valid_acc_list  = []\n",
    "\n",
    "    for epoch in range(epoch_num):\n",
    "\n",
    "        train_loss = 0.0\n",
    "        train_acc  = 0.0\n",
    "        valid_loss = 0.0\n",
    "        valid_acc  = 0.0\n",
    "\n",
    "        # train====================\n",
    "        net.train()\n",
    "        for xx, yy in train_loader:\n",
    "            xx, yy = xx.long().to(device), yy.to(device)\n",
    "\n",
    "            net.batch_size = len(yy)\n",
    "            net.hidden = net.init_hidden()\n",
    "\n",
    "            optimizer.zero_grad()    # 勾配の初期化\n",
    "\n",
    "            output = net(xx)\n",
    "            loss   = criterion(output, yy)\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            train_acc += (output.max(1)[1] == yy).sum().item()\n",
    "\n",
    "            loss.backward(retain_graph=True)     # 逆伝播の計算\n",
    "            optimizer.step()    # 勾配の更新\n",
    "        avg_train_loss = train_loss / len(train_loader.dataset)\n",
    "        avg_train_acc  = train_acc  / len(train_loader.dataset)\n",
    "\n",
    "        # eval========================\n",
    "        net.eval()\n",
    "        y_pred = []\n",
    "        with torch.no_grad():\n",
    "            for xx, yy in valid_loader:\n",
    "                xx, yy = xx.long().to(device), yy.to(device)\n",
    "\n",
    "                net.batch_size = len(yy)\n",
    "                net.hidden = net.init_hidden()\n",
    "\n",
    "                output = net(xx)\n",
    "                loss   = criterion(output, yy)\n",
    "\n",
    "                valid_loss += loss.item()\n",
    "                valid_acc  += (output.max(1)[1] == yy).sum().item()\n",
    "                y_pred += output.data.max(1, keepdim=True)[1].to('cpu').numpy()[:,0].tolist()\n",
    "                \n",
    "        avg_valid_loss = valid_loss / len(valid_loader.dataset)\n",
    "        avg_valid_acc  = valid_acc  / len(valid_loader.dataset)\n",
    "        \n",
    "        if epoch in [49, 99, 199, 299]:\n",
    "            result = precision_recall_fscore_support(y_valid, y_pred, average='macro')\n",
    "            print(\"At {} epoch, validation scores are...\".format(epoch+1))\n",
    "            print('Acc: {}, Precision: {}, Recall: {}, F1: {}'.format(avg_valid_acc, result[0], result[1], result[2]))\n",
    "            print('-----------')\n",
    "\n",
    "        train_loss_list.append(avg_train_loss)\n",
    "        train_acc_list.append(avg_train_acc)\n",
    "        valid_loss_list.append(avg_valid_loss)\n",
    "        valid_acc_list.append(avg_valid_acc)\n",
    "        \n",
    "    training_plot(epoch_num, train_loss_list, train_acc_list, valid_loss_list, valid_acc_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(net, test_loader, y_test):\n",
    "    net.eval()\n",
    "    y_pred = []\n",
    "    with torch.no_grad():\n",
    "        for xx, yy in test_loader:\n",
    "            xx, yy = xx.long().to(device), yy.to(device)\n",
    "\n",
    "            net.batch_size = len(yy)\n",
    "            net.hidden = net.init_hidden()\n",
    "\n",
    "            output = net(xx)\n",
    "            y_pred += output.data.max(1, keepdim=True)[1].to('cpu').numpy()[:,0].tolist()\n",
    "\n",
    "    acc = (y_pred == y_test).sum().item() / len(y_test)\n",
    "    result = precision_recall_fscore_support(y_test, y_pred, average='macro')\n",
    "    return [acc, result[0], result[1], result[2]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(2019)\n",
    "np.random.RandomState(2019)\n",
    "torch.manual_seed(2019)\n",
    "\n",
    "epoch_num  = 100     # エポック数\n",
    "batch_size = 64      # バッチサイズ\n",
    "vocab_size = len(vocab_idx) # 語彙数\n",
    "emb_dim    = 200     # 分散表現の次元数\n",
    "hidden_dim = 100     # 隠れ層の次元数\n",
    "drop_rate  = 0.0     # Dropout率\n",
    "actiate    = 'tanh'  # 活性化関数\n",
    "\n",
    "learningrate = 0.1  # 学習率\n",
    "weight_decay = 1e-3  # l2正則化\n",
    "\n",
    "# dataloader(注意：validloader, testloaderのshuffleはFalse)\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "# valid_loader = DataLoader(valid_ds, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "test_loader  = DataLoader(test_ds, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "net = LSTM(batch_size=batch_size, vocab_size=vocab_size, emb_dim=emb_dim, hidden_dim=hidden_dim, dropout_rate=drop_rate, activate=actiate, bidirectional=True).to(device)\n",
    "\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "criterion = nn.NLLLoss()\n",
    "# optimizer = optim.Adam(net.parameters(), lr=learningrate, weight_decay=weight_decay)\n",
    "optimizer = optim.SGD(net.parameters(), lr=learningrate, weight_decay=weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM(\n",
      "  (emb): Embedding(21100, 200)\n",
      "  (lstm): LSTM(200, 100, batch_first=True, bidirectional=True)\n",
      "  (fc0): Linear(in_features=200, out_features=100, bias=True)\n",
      "  (fc1): Linear(in_features=100, out_features=2, bias=True)\n",
      "  (do): Dropout(p=0.0)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/b2018yniki/.pyenv/versions/3.7.1/lib/python3.7/site-packages/torch/nn/functional.py:1374: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "/home/b2018yniki/.pyenv/versions/3.7.1/lib/python3.7/site-packages/ipykernel_launcher.py:31: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At 50 epoch, validation scores are...\n",
      "Acc: 0.9458962090541038, Precision: 0.8762861461619348, Recall: 0.8653412745408324, F1: 0.8706988398801517\n",
      "-----------\n"
     ]
    }
   ],
   "source": [
    "training(net, train_loader, test_loader, epoch_num, test_df.causal_flag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = test(net, test_loader, test_df.causal_flag)\n",
    "print('Accuracy: {}, Precision: {}, Recall: {}, F1: {}'.format(result[0], result[1], result[2], result[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
