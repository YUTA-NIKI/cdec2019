{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "import MeCab\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pytorch version:  1.1.0\n",
      "Currently selected device:  0\n",
      "# GPUs available:  1\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "print('Pytorch version: ', torch.__version__)\n",
    "print('Currently selected device: ', torch.cuda.current_device())\n",
    "print('# GPUs available: ', torch.cuda.device_count())\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = torch.device('cpu') # デバッグ用\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset作成に必要なclassおよびfunction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 分かち書き"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_path = '-d /usr/lib/x86_64-linux-gnu/mecab/dic/mecab-ipadic-neologd'\n",
    "\n",
    "def get_tango(sen):\n",
    "    word_list = []\n",
    "    tagger = MeCab.Tagger(dict_path)\n",
    "    for word_line in tagger.parse(sen).split(\"\\n\"):\n",
    "        if word_line.strip() == \"EOS\":\n",
    "            break\n",
    "        (word, temp) = word_line.split(\"\\t\")\n",
    "        temps = temp.split(',')\n",
    "        if \"記号\" == temps[0]:\n",
    "            continue\n",
    "        if \"数\" == temps[1]:\n",
    "            continue\n",
    "        word_list.append(word)\n",
    "    return word_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## torknize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df2input(df, vocab_idx):\n",
    "    data = []\n",
    "    for text in df.values:\n",
    "        words = get_tango(text)\n",
    "        data.append([vocab_idx[word] for word in words if word in vocab_idx.keys()])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data, tags):\n",
    "        super(MyDataset, self).__init__()\n",
    "        assert len(data) == len(tags)\n",
    "        # npに変換し、0埋めを行う\n",
    "        max_length = max([len(d) for d in data])\n",
    "        self.data = np.zeros((len(tags), max_length))\n",
    "        for i, d1 in enumerate(data):\n",
    "            for l, d2 in enumerate(d1):\n",
    "                self.data[i][l] = d2\n",
    "        self.tags = tags\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tags)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index], self.tags[index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM \\w self-attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ATT(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super(ATT, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.fc = nn.Linear(hidden_dim, 1)\n",
    "    def forward(self, inputs):\n",
    "        b_size = inputs.size(0)\n",
    "        inputs = inputs.contiguous().view(-1, self.hidden_dim)\n",
    "        att = self.fc(torch.tanh(inputs))\n",
    "        return F.softmax(att.view(b_size, -1), dim=1).unsqueeze(2)\n",
    "    \n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, batch_size, vocab_size, emb_dim, hidden_dim, dropout_rate=0.0, activate='tanh', bidirectional=False, device='cpu'):\n",
    "        super(LSTM, self).__init__()\n",
    "        \n",
    "        self.vocab_size = vocab_size\n",
    "        self.emb_dim    = emb_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.batch_size = batch_size\n",
    "        self.bidirectional = bidirectional\n",
    "        self.activate   = activate\n",
    "        \n",
    "        self.emb  = nn.Embedding(self.vocab_size, self.emb_dim)\n",
    "        self.lstm = nn.LSTM(self.emb_dim, self.hidden_dim, batch_first=True, bidirectional=self.bidirectional)\n",
    "        self.att = ATT(hidden_dim * 2)\n",
    "        \n",
    "        self.fc0 = nn.Linear(hidden_dim * 2, 100)\n",
    "        self.fc1 = nn.Linear(100, 2)\n",
    "        self.do  = nn.Dropout(dropout_rate)\n",
    "        self.device = device\n",
    "        self.hidden = self.init_hidden()\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.emb(x)\n",
    "        lstm_out, self.hidden = self.lstm(x, self.hidden)\n",
    "        \n",
    "        att = self.att(lstm_out)\n",
    "        feats = (lstm_out * att).sum(dim=1) # (b, s, h) -> (b, h)\n",
    "        \n",
    "        y = self.fc0(feats)\n",
    "        y = self.do(y)\n",
    "        if self.activate == 'tanh':\n",
    "            y = self.fc1(F.tanh(y))\n",
    "        elif self.activate == 'relu':\n",
    "            y = self.fc1(F.relu(y))\n",
    "        tag_scores = F.log_softmax(y)\n",
    "        return tag_scores\n",
    "\n",
    "    def init_hidden(self):\n",
    "        # The axes semantics are (num_layers, minibatch_size, hidden_dim)\n",
    "        num = 2 if self.bidirectional else 1    # bidirectionalのとき2\n",
    "        h0 = torch.zeros(num, self.batch_size, self.hidden_dim).to(self.device)\n",
    "        c0 = torch.zeros(num, self.batch_size, self.hidden_dim).to(self.device)\n",
    "        return (h0, c0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(net, train_loader, epoch_num):\n",
    "\n",
    "    for epoch in range(epoch_num):\n",
    "\n",
    "        train_loss = 0.0\n",
    "        train_acc  = 0.0\n",
    "\n",
    "        # train====================\n",
    "        net.train()\n",
    "        for xx, yy in train_loader:\n",
    "            xx, yy = xx.long().to(device), yy.to(device)\n",
    "\n",
    "            net.batch_size = len(yy)\n",
    "            net.hidden = net.init_hidden()\n",
    "\n",
    "            optimizer.zero_grad()    # 勾配の初期化\n",
    "\n",
    "            output = net(xx)\n",
    "            loss   = criterion(output, yy)\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            train_acc += (output.max(1)[1] == yy).sum().item()\n",
    "\n",
    "            loss.backward(retain_graph=True)     # 逆伝播の計算\n",
    "            optimizer.step()    # 勾配の更新"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(net, test_loader, y_test):\n",
    "    net.eval()\n",
    "    y_pred = []\n",
    "    with torch.no_grad():\n",
    "        for xx, yy in test_loader:\n",
    "            xx, yy = xx.long().to(device), yy.to(device)\n",
    "\n",
    "            net.batch_size = len(yy)\n",
    "            net.hidden = net.init_hidden()\n",
    "\n",
    "            output = net(xx)\n",
    "            y_pred += output.data.max(1, keepdim=True)[1].to('cpu').numpy()[:,0].tolist()\n",
    "\n",
    "    acc = (y_pred == y_test).sum().item() / len(y_test)\n",
    "    result = precision_recall_fscore_support(y_test, y_pred, average='macro')\n",
    "    return [acc, result[0], result[1], result[2]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results of GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>embedding_dim</th>\n",
       "      <th>hidden_dim</th>\n",
       "      <th>activate_func</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>l2_regular</th>\n",
       "      <th>dropout_rate</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>218</th>\n",
       "      <td>300</td>\n",
       "      <td>64</td>\n",
       "      <td>300</td>\n",
       "      <td>100</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.834331</td>\n",
       "      <td>0.830647</td>\n",
       "      <td>0.831035</td>\n",
       "      <td>0.830838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>281</th>\n",
       "      <td>300</td>\n",
       "      <td>64</td>\n",
       "      <td>300</td>\n",
       "      <td>200</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.834331</td>\n",
       "      <td>0.831075</td>\n",
       "      <td>0.829846</td>\n",
       "      <td>0.830429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>100</td>\n",
       "      <td>64</td>\n",
       "      <td>300</td>\n",
       "      <td>100</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.834331</td>\n",
       "      <td>0.831682</td>\n",
       "      <td>0.828658</td>\n",
       "      <td>0.829997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244</th>\n",
       "      <td>200</td>\n",
       "      <td>64</td>\n",
       "      <td>300</td>\n",
       "      <td>200</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.832335</td>\n",
       "      <td>0.828332</td>\n",
       "      <td>0.830481</td>\n",
       "      <td>0.829282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>300</td>\n",
       "      <td>64</td>\n",
       "      <td>300</td>\n",
       "      <td>200</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.832335</td>\n",
       "      <td>0.828533</td>\n",
       "      <td>0.829293</td>\n",
       "      <td>0.828899</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     epoch  batch_size  embedding_dim  hidden_dim activate_func  \\\n",
       "218    300          64            300         100          tanh   \n",
       "281    300          64            300         200          relu   \n",
       "195    100          64            300         100          tanh   \n",
       "244    200          64            300         200          tanh   \n",
       "245    300          64            300         200          tanh   \n",
       "\n",
       "     learning_rate  l2_regular  dropout_rate  accuracy  precision    recall  \\\n",
       "218            0.1       0.000           0.5  0.834331   0.830647  0.831035   \n",
       "281            0.1       0.001           0.5  0.834331   0.831075  0.829846   \n",
       "195            0.1       0.001           0.0  0.834331   0.831682  0.828658   \n",
       "244            0.1       0.001           0.0  0.832335   0.828332  0.830481   \n",
       "245            0.1       0.001           0.0  0.832335   0.828533  0.829293   \n",
       "\n",
       "           f1  \n",
       "218  0.830838  \n",
       "281  0.830429  \n",
       "195  0.829997  \n",
       "244  0.829282  \n",
       "245  0.828899  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_df = pd.read_csv('results/gridsearch_lstm_attention_end2end.csv').sort_values(by=['f1'], ascending=False)\n",
    "gs_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>embedding_dim</th>\n",
       "      <th>hidden_dim</th>\n",
       "      <th>activate_func</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>l2_regular</th>\n",
       "      <th>dropout_rate</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>281</th>\n",
       "      <td>300</td>\n",
       "      <td>64</td>\n",
       "      <td>300</td>\n",
       "      <td>200</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.834331</td>\n",
       "      <td>0.831075</td>\n",
       "      <td>0.829846</td>\n",
       "      <td>0.830429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218</th>\n",
       "      <td>300</td>\n",
       "      <td>64</td>\n",
       "      <td>300</td>\n",
       "      <td>100</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.834331</td>\n",
       "      <td>0.830647</td>\n",
       "      <td>0.831035</td>\n",
       "      <td>0.830838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>100</td>\n",
       "      <td>64</td>\n",
       "      <td>300</td>\n",
       "      <td>100</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.834331</td>\n",
       "      <td>0.831682</td>\n",
       "      <td>0.828658</td>\n",
       "      <td>0.829997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>268</th>\n",
       "      <td>200</td>\n",
       "      <td>64</td>\n",
       "      <td>300</td>\n",
       "      <td>200</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.832335</td>\n",
       "      <td>0.829458</td>\n",
       "      <td>0.826916</td>\n",
       "      <td>0.828060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217</th>\n",
       "      <td>200</td>\n",
       "      <td>64</td>\n",
       "      <td>300</td>\n",
       "      <td>100</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.832335</td>\n",
       "      <td>0.828698</td>\n",
       "      <td>0.828698</td>\n",
       "      <td>0.828698</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     epoch  batch_size  embedding_dim  hidden_dim activate_func  \\\n",
       "281    300          64            300         200          relu   \n",
       "218    300          64            300         100          tanh   \n",
       "195    100          64            300         100          tanh   \n",
       "268    200          64            300         200          tanh   \n",
       "217    200          64            300         100          tanh   \n",
       "\n",
       "     learning_rate  l2_regular  dropout_rate  accuracy  precision    recall  \\\n",
       "281            0.1       0.001           0.5  0.834331   0.831075  0.829846   \n",
       "218            0.1       0.000           0.5  0.834331   0.830647  0.831035   \n",
       "195            0.1       0.001           0.0  0.834331   0.831682  0.828658   \n",
       "268            0.1       0.001           0.5  0.832335   0.829458  0.826916   \n",
       "217            0.1       0.000           0.5  0.832335   0.828698  0.828698   \n",
       "\n",
       "           f1  \n",
       "281  0.830429  \n",
       "218  0.830838  \n",
       "195  0.829997  \n",
       "268  0.828060  \n",
       "217  0.828698  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_df = pd.read_csv('results/gridsearch_lstm_attention_end2end.csv').sort_values(by=['accuracy'], ascending=False)\n",
    "gs_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Condition1\n",
    "the random seed of train_test_split is 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulaly size: 9746\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv('/home/b2018yniki/data/nikkei/train.txt', sep='\\t', header=None, names=['target', 'time', 'body']).drop('time', axis=1)\n",
    "test_df  = pd.read_csv('/home/b2018yniki/data/nikkei/test.txt', sep='\\t', header=None, names=['target', 'time', 'body']).drop('time', axis=1)\n",
    "total_df = pd.concat([train_df, test_df], ignore_index=True).drop_duplicates()\n",
    "\n",
    "X = total_df.body\n",
    "y = total_df.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=2019)\n",
    "del train_df, test_df, total_df, X, y\n",
    "\n",
    "vocab = []\n",
    "for text in X_train.values:\n",
    "    vocab.extend(get_tango(text))\n",
    "vocab = list(set(vocab))\n",
    "print('vocabulaly size: {}'.format(len(vocab)))\n",
    "vocab_idx = dict(zip(vocab, range(len(vocab))))\n",
    "del vocab\n",
    "\n",
    "X_train = df2input(X_train, vocab_idx)\n",
    "X_test  = df2input(X_test, vocab_idx)\n",
    "\n",
    "train_ds = MyDataset(X_train, y_train.values)\n",
    "test_ds  = MyDataset(X_test, y_test.values)\n",
    "\n",
    "del X_train, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM(\n",
      "  (emb): Embedding(9746, 300)\n",
      "  (lstm): LSTM(300, 200, batch_first=True, bidirectional=True)\n",
      "  (att): ATT(\n",
      "    (fc): Linear(in_features=400, out_features=1, bias=True)\n",
      "  )\n",
      "  (fc0): Linear(in_features=400, out_features=100, bias=True)\n",
      "  (fc1): Linear(in_features=100, out_features=2, bias=True)\n",
      "  (do): Dropout(p=0.5)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/b2018yniki/.pyenv/versions/3.7.2/lib/python3.7/site-packages/ipykernel_launcher.py:47: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8183632734530938, Precision: 0.8141402714932127, Recall: 0.8165033052199681, F1: 0.8151553852703277\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(2019)\n",
    "np.random.RandomState(2019)\n",
    "torch.manual_seed(2019)\n",
    "\n",
    "# hyperparameter\n",
    "epoch      = 300\n",
    "batch_size = 64\n",
    "vocab_size = len(vocab_idx)\n",
    "emb_dim    = 300\n",
    "hidden_dim = 200\n",
    "activate   = 'relu'\n",
    "drop_rate  = 0.5\n",
    "lr = 0.1\n",
    "l2 = 0.001\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "test_loader  = DataLoader(test_ds, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "np.random.seed(2019)\n",
    "np.random.RandomState(2019)\n",
    "torch.manual_seed(2019)\n",
    "\n",
    "net = LSTM(batch_size, vocab_size, emb_dim, hidden_dim, drop_rate, activate, bidirectional=True, device=device).to(device)\n",
    "\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=lr, weight_decay=l2)\n",
    "\n",
    "print(net)\n",
    "\n",
    "training(net, train_loader, epoch)\n",
    "result = test(net, test_loader, y_test)\n",
    "print('Accuracy: {}, Precision: {}, Recall: {}, F1: {}'.format(result[0], result[1], result[2], result[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net.state_dict(), 'best_params/lstm_att_s2019.prm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "del net, criterion, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8183632734530938, Precision: 0.8141402714932127, Recall: 0.8165033052199681, F1: 0.8151553852703277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/b2018yniki/.pyenv/versions/3.7.2/lib/python3.7/site-packages/ipykernel_launcher.py:47: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    }
   ],
   "source": [
    "net = LSTM(batch_size, vocab_size, emb_dim, hidden_dim, drop_rate, activate, bidirectional=True, device=device).to(device)\n",
    "net.load_state_dict(torch.load('best_params/lstm_att_s2019.prm'))\n",
    "result = test(net, test_loader, y_test)\n",
    "print('Accuracy: {}, Precision: {}, Recall: {}, F1: {}'.format(result[0], result[1], result[2], result[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "del net, train_ds, test_ds, train_loader, test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Condition2\n",
    "the random seed of train_test_split is 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulaly size: 9741\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv('/home/b2018yniki/data/nikkei/train.txt', sep='\\t', header=None, names=['target', 'time', 'body']).drop('time', axis=1)\n",
    "test_df  = pd.read_csv('/home/b2018yniki/data/nikkei/test.txt', sep='\\t', header=None, names=['target', 'time', 'body']).drop('time', axis=1)\n",
    "total_df = pd.concat([train_df, test_df], ignore_index=True).drop_duplicates()\n",
    "\n",
    "X = total_df.body\n",
    "y = total_df.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=2020)\n",
    "del train_df, test_df, total_df, X, y\n",
    "\n",
    "vocab = []\n",
    "for text in X_train.values:\n",
    "    vocab.extend(get_tango(text))\n",
    "vocab = list(set(vocab))\n",
    "print('vocabulaly size: {}'.format(len(vocab)))\n",
    "vocab_idx = dict(zip(vocab, range(len(vocab))))\n",
    "del vocab\n",
    "\n",
    "X_train = df2input(X_train, vocab_idx)\n",
    "X_test  = df2input(X_test, vocab_idx)\n",
    "\n",
    "train_ds = MyDataset(X_train, y_train.values)\n",
    "test_ds  = MyDataset(X_test, y_test.values)\n",
    "\n",
    "del X_train, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM(\n",
      "  (emb): Embedding(9746, 300)\n",
      "  (lstm): LSTM(300, 200, batch_first=True, bidirectional=True)\n",
      "  (att): ATT(\n",
      "    (fc): Linear(in_features=400, out_features=1, bias=True)\n",
      "  )\n",
      "  (fc0): Linear(in_features=400, out_features=100, bias=True)\n",
      "  (fc1): Linear(in_features=100, out_features=2, bias=True)\n",
      "  (do): Dropout(p=0.5)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/b2018yniki/.pyenv/versions/3.7.2/lib/python3.7/site-packages/ipykernel_launcher.py:47: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8203592814371258, Precision: 0.8175164947793223, Recall: 0.8210434275721363, F1: 0.8186242960579244\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(2019)\n",
    "np.random.RandomState(2019)\n",
    "torch.manual_seed(2019)\n",
    "\n",
    "# hyperparameter\n",
    "epoch      = 300\n",
    "batch_size = 64\n",
    "vocab_size = len(vocab_idx)\n",
    "emb_dim    = 300\n",
    "hidden_dim = 200\n",
    "activate   = 'relu'\n",
    "drop_rate  = 0.5\n",
    "lr = 0.1\n",
    "l2 = 0.001\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "test_loader  = DataLoader(test_ds, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "np.random.seed(2019)\n",
    "np.random.RandomState(2019)\n",
    "torch.manual_seed(2019)\n",
    "\n",
    "net = LSTM(batch_size, vocab_size, emb_dim, hidden_dim, drop_rate, activate, bidirectional=True, device=device).to(device)\n",
    "\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=lr, weight_decay=l2)\n",
    "\n",
    "print(net)\n",
    "\n",
    "training(net, train_loader, epoch)\n",
    "result = test(net, test_loader, y_test)\n",
    "print('Accuracy: {}, Precision: {}, Recall: {}, F1: {}'.format(result[0], result[1], result[2], result[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net.state_dict(), 'best_params/lstm_att_s2020.prm')\n",
    "del net, criterion, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/b2018yniki/.pyenv/versions/3.7.2/lib/python3.7/site-packages/ipykernel_launcher.py:47: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8203592814371258, Precision: 0.8175164947793223, Recall: 0.8210434275721363, F1: 0.8186242960579244\n"
     ]
    }
   ],
   "source": [
    "net = LSTM(batch_size, vocab_size, emb_dim, hidden_dim, drop_rate, activate, bidirectional=True, device=device).to(device)\n",
    "net.load_state_dict(torch.load('best_params/lstm_att_s2020.prm'))\n",
    "result = test(net, test_loader, y_test)\n",
    "print('Accuracy: {}, Precision: {}, Recall: {}, F1: {}'.format(result[0], result[1], result[2], result[3]))\n",
    "\n",
    "del net, train_ds, test_ds, train_loader, test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Condition3\n",
    "the random seed of train_test_split is 1996"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulaly size: 9647\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv('/home/b2018yniki/data/nikkei/train.txt', sep='\\t', header=None, names=['target', 'time', 'body']).drop('time', axis=1)\n",
    "test_df  = pd.read_csv('/home/b2018yniki/data/nikkei/test.txt', sep='\\t', header=None, names=['target', 'time', 'body']).drop('time', axis=1)\n",
    "total_df = pd.concat([train_df, test_df], ignore_index=True).drop_duplicates()\n",
    "\n",
    "X = total_df.body\n",
    "y = total_df.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=1996)\n",
    "del train_df, test_df, total_df, X, y\n",
    "\n",
    "vocab = []\n",
    "for text in X_train.values:\n",
    "    vocab.extend(get_tango(text))\n",
    "vocab = list(set(vocab))\n",
    "print('vocabulaly size: {}'.format(len(vocab)))\n",
    "vocab_idx = dict(zip(vocab, range(len(vocab))))\n",
    "del vocab\n",
    "\n",
    "X_train = df2input(X_train, vocab_idx)\n",
    "X_test  = df2input(X_test, vocab_idx)\n",
    "\n",
    "train_ds = MyDataset(X_train, y_train.values)\n",
    "test_ds  = MyDataset(X_test, y_test.values)\n",
    "\n",
    "del X_train, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM(\n",
      "  (emb): Embedding(9647, 300)\n",
      "  (lstm): LSTM(300, 200, batch_first=True, bidirectional=True)\n",
      "  (att): ATT(\n",
      "    (fc): Linear(in_features=400, out_features=1, bias=True)\n",
      "  )\n",
      "  (fc0): Linear(in_features=400, out_features=100, bias=True)\n",
      "  (fc1): Linear(in_features=100, out_features=2, bias=True)\n",
      "  (do): Dropout(p=0.5)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/b2018yniki/.pyenv/versions/3.7.2/lib/python3.7/site-packages/ipykernel_launcher.py:47: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7984031936127745, Precision: 0.793888604025125, Recall: 0.8005341810316576, F1: 0.7954697030327282\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(2019)\n",
    "np.random.RandomState(2019)\n",
    "torch.manual_seed(2019)\n",
    "\n",
    "# hyperparameter\n",
    "epoch      = 300\n",
    "batch_size = 64\n",
    "vocab_size = len(vocab_idx)\n",
    "emb_dim    = 300\n",
    "hidden_dim = 200\n",
    "activate   = 'relu'\n",
    "drop_rate  = 0.5\n",
    "lr = 0.1\n",
    "l2 = 0.001\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "test_loader  = DataLoader(test_ds, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "np.random.seed(2019)\n",
    "np.random.RandomState(2019)\n",
    "torch.manual_seed(2019)\n",
    "\n",
    "net = LSTM(batch_size, vocab_size, emb_dim, hidden_dim, drop_rate, activate, bidirectional=True, device=device).to(device)\n",
    "\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=lr, weight_decay=l2)\n",
    "\n",
    "print(net)\n",
    "\n",
    "training(net, train_loader, epoch)\n",
    "result = test(net, test_loader, y_test)\n",
    "print('Accuracy: {}, Precision: {}, Recall: {}, F1: {}'.format(result[0], result[1], result[2], result[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/b2018yniki/.pyenv/versions/3.7.2/lib/python3.7/site-packages/ipykernel_launcher.py:47: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7984031936127745, Precision: 0.793888604025125, Recall: 0.8005341810316576, F1: 0.7954697030327282\n"
     ]
    }
   ],
   "source": [
    "torch.save(net.state_dict(), 'best_params/lstm_att_s1996.prm')\n",
    "del net, criterion, optimizer\n",
    "\n",
    "net = LSTM(batch_size, vocab_size, emb_dim, hidden_dim, drop_rate, activate, bidirectional=True, device=device).to(device)\n",
    "net.load_state_dict(torch.load('best_params/lstm_att_s1996.prm'))\n",
    "result = test(net, test_loader, y_test)\n",
    "print('Accuracy: {}, Precision: {}, Recall: {}, F1: {}'.format(result[0], result[1], result[2], result[3]))\n",
    "\n",
    "del net, train_ds, test_ds, train_loader, test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Condition4\n",
    "the random seed of train_test_split is 1192"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulaly size: 9773\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv('/home/b2018yniki/data/nikkei/train.txt', sep='\\t', header=None, names=['target', 'time', 'body']).drop('time', axis=1)\n",
    "test_df  = pd.read_csv('/home/b2018yniki/data/nikkei/test.txt', sep='\\t', header=None, names=['target', 'time', 'body']).drop('time', axis=1)\n",
    "total_df = pd.concat([train_df, test_df], ignore_index=True).drop_duplicates()\n",
    "\n",
    "X = total_df.body\n",
    "y = total_df.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=1192)\n",
    "del train_df, test_df, total_df, X, y\n",
    "\n",
    "vocab = []\n",
    "for text in X_train.values:\n",
    "    vocab.extend(get_tango(text))\n",
    "vocab = list(set(vocab))\n",
    "print('vocabulaly size: {}'.format(len(vocab)))\n",
    "vocab_idx = dict(zip(vocab, range(len(vocab))))\n",
    "del vocab\n",
    "\n",
    "X_train = df2input(X_train, vocab_idx)\n",
    "X_test  = df2input(X_test, vocab_idx)\n",
    "\n",
    "train_ds = MyDataset(X_train, y_train.values)\n",
    "test_ds  = MyDataset(X_test, y_test.values)\n",
    "\n",
    "del X_train, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM(\n",
      "  (emb): Embedding(9773, 300)\n",
      "  (lstm): LSTM(300, 200, batch_first=True, bidirectional=True)\n",
      "  (att): ATT(\n",
      "    (fc): Linear(in_features=400, out_features=1, bias=True)\n",
      "  )\n",
      "  (fc0): Linear(in_features=400, out_features=100, bias=True)\n",
      "  (fc1): Linear(in_features=100, out_features=2, bias=True)\n",
      "  (do): Dropout(p=0.5)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/b2018yniki/.pyenv/versions/3.7.2/lib/python3.7/site-packages/ipykernel_launcher.py:47: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8183632734530938, Precision: 0.8134097319026161, Recall: 0.8159911750285995, F1: 0.8145327621767412\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(2019)\n",
    "np.random.RandomState(2019)\n",
    "torch.manual_seed(2019)\n",
    "\n",
    "# hyperparameter\n",
    "epoch      = 300\n",
    "batch_size = 64\n",
    "vocab_size = len(vocab_idx)\n",
    "emb_dim    = 300\n",
    "hidden_dim = 200\n",
    "activate   = 'relu'\n",
    "drop_rate  = 0.5\n",
    "lr = 0.1\n",
    "l2 = 0.001\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "test_loader  = DataLoader(test_ds, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "np.random.seed(2019)\n",
    "np.random.RandomState(2019)\n",
    "torch.manual_seed(2019)\n",
    "\n",
    "net = LSTM(batch_size, vocab_size, emb_dim, hidden_dim, drop_rate, activate, bidirectional=True, device=device).to(device)\n",
    "\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=lr, weight_decay=l2)\n",
    "\n",
    "print(net)\n",
    "\n",
    "training(net, train_loader, epoch)\n",
    "result = test(net, test_loader, y_test)\n",
    "print('Accuracy: {}, Precision: {}, Recall: {}, F1: {}'.format(result[0], result[1], result[2], result[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8183632734530938, Precision: 0.8134097319026161, Recall: 0.8159911750285995, F1: 0.8145327621767412\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/b2018yniki/.pyenv/versions/3.7.2/lib/python3.7/site-packages/ipykernel_launcher.py:47: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    }
   ],
   "source": [
    "torch.save(net.state_dict(), 'best_params/lstm_att_s1192.prm')\n",
    "del net, criterion, optimizer, vocab_idx\n",
    "\n",
    "net = LSTM(batch_size, vocab_size, emb_dim, hidden_dim, drop_rate, activate, bidirectional=True, device=device).to(device)\n",
    "net.load_state_dict(torch.load('best_params/lstm_att_s1192.prm'))\n",
    "result = test(net, test_loader, y_test)\n",
    "print('Accuracy: {}, Precision: {}, Recall: {}, F1: {}'.format(result[0], result[1], result[2], result[3]))\n",
    "\n",
    "del net, train_ds, test_ds, train_loader, test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Condition5\n",
    "the random seed of train_test_split is 794"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulaly size: 9708\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv('/home/b2018yniki/data/nikkei/train.txt', sep='\\t', header=None, names=['target', 'time', 'body']).drop('time', axis=1)\n",
    "test_df  = pd.read_csv('/home/b2018yniki/data/nikkei/test.txt', sep='\\t', header=None, names=['target', 'time', 'body']).drop('time', axis=1)\n",
    "total_df = pd.concat([train_df, test_df], ignore_index=True).drop_duplicates()\n",
    "\n",
    "X = total_df.body\n",
    "y = total_df.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=794)\n",
    "del train_df, test_df, total_df, X, y\n",
    "\n",
    "vocab = []\n",
    "for text in X_train.values:\n",
    "    vocab.extend(get_tango(text))\n",
    "vocab = list(set(vocab))\n",
    "print('vocabulaly size: {}'.format(len(vocab)))\n",
    "vocab_idx = dict(zip(vocab, range(len(vocab))))\n",
    "del vocab\n",
    "\n",
    "X_train = df2input(X_train, vocab_idx)\n",
    "X_test  = df2input(X_test, vocab_idx)\n",
    "\n",
    "train_ds = MyDataset(X_train, y_train.values)\n",
    "test_ds  = MyDataset(X_test, y_test.values)\n",
    "\n",
    "del X_train, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM(\n",
      "  (emb): Embedding(9708, 300)\n",
      "  (lstm): LSTM(300, 200, batch_first=True, bidirectional=True)\n",
      "  (att): ATT(\n",
      "    (fc): Linear(in_features=400, out_features=1, bias=True)\n",
      "  )\n",
      "  (fc0): Linear(in_features=400, out_features=100, bias=True)\n",
      "  (fc1): Linear(in_features=100, out_features=2, bias=True)\n",
      "  (do): Dropout(p=0.5)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/b2018yniki/.pyenv/versions/3.7.2/lib/python3.7/site-packages/ipykernel_launcher.py:47: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(2019)\n",
    "np.random.RandomState(2019)\n",
    "torch.manual_seed(2019)\n",
    "\n",
    "# hyperparameter\n",
    "epoch      = 300\n",
    "batch_size = 64\n",
    "vocab_size = len(vocab_idx)\n",
    "emb_dim    = 300\n",
    "hidden_dim = 200\n",
    "activate   = 'relu'\n",
    "drop_rate  = 0.5\n",
    "lr = 0.1\n",
    "l2 = 0.001\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "test_loader  = DataLoader(test_ds, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "np.random.seed(2019)\n",
    "np.random.RandomState(2019)\n",
    "torch.manual_seed(2019)\n",
    "\n",
    "net = LSTM(batch_size, vocab_size, emb_dim, hidden_dim, drop_rate, activate, bidirectional=True, device=device).to(device)\n",
    "\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=lr, weight_decay=l2)\n",
    "\n",
    "print(net)\n",
    "\n",
    "training(net, train_loader, epoch)\n",
    "result = test(net, test_loader, y_test)\n",
    "print('Accuracy: {}, Precision: {}, Recall: {}, F1: {}'.format(result[0], result[1], result[2], result[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
