{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "import MeCab\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pytorch version:  1.1.0\n",
      "Currently selected device:  0\n",
      "# GPUs available:  2\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "print('Pytorch version: ', torch.__version__)\n",
    "print('Currently selected device: ', torch.cuda.current_device())\n",
    "print('# GPUs available: ', torch.cuda.device_count())\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = torch.device('cpu') # デバッグ用\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset作成に必要なclassおよびfunction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 分かち書き"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_path = '-d /usr/local/lib/mecab/dic/mecab-ipadic-neologd'\n",
    "\n",
    "def get_tango(sen):\n",
    "    word_list = []\n",
    "    tagger = MeCab.Tagger(dict_path)\n",
    "    for word_line in tagger.parse(sen).split(\"\\n\"):\n",
    "        if word_line.strip() == \"EOS\":\n",
    "            break\n",
    "        (word, temp) = word_line.split(\"\\t\")\n",
    "        temps = temp.split(',')\n",
    "        if \"記号\" == temps[0]:\n",
    "            continue\n",
    "        if \"数\" == temps[1]:\n",
    "            continue\n",
    "        word_list.append(word)\n",
    "    return word_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## torknize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df2input(df, vocab_idx):\n",
    "    data = []\n",
    "    for text in df.values:\n",
    "        words = get_tango(text)\n",
    "        data.append([vocab_idx[word] for word in words if word in vocab_idx.keys()])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data, tags):\n",
    "        super(MyDataset, self).__init__()\n",
    "        assert len(data) == len(tags)\n",
    "        # npに変換し、0埋めを行う\n",
    "        max_length = max([len(d) for d in data])\n",
    "        self.data = np.zeros((len(tags), max_length))\n",
    "        for i, d1 in enumerate(data):\n",
    "            for l, d2 in enumerate(d1):\n",
    "                self.data[i][l] = d2\n",
    "        self.tags = tags\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tags)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index], self.tags[index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, batch_size, vocab_size, emb_dim, hidden_dim, dropout_rate=0.0, activate='tanh', bidirectional=False, device='cpu'):\n",
    "        super(LSTM, self).__init__()\n",
    "        \n",
    "        self.vocab_size = vocab_size\n",
    "        self.emb_dim    = emb_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.batch_size = batch_size\n",
    "        self.bidirectional = bidirectional\n",
    "        self.activate   = activate\n",
    "        \n",
    "        self.emb  = nn.Embedding(self.vocab_size, self.emb_dim)\n",
    "        self.lstm = nn.LSTM(self.emb_dim, self.hidden_dim, batch_first=True, bidirectional=self.bidirectional)\n",
    "        \n",
    "        self.fc0 = nn.Linear(hidden_dim * 2, 100)\n",
    "        self.fc1 = nn.Linear(100, 2)\n",
    "        self.do  = nn.Dropout(dropout_rate)\n",
    "        self.device = device\n",
    "        self.hidden = self.init_hidden()\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.emb(x)\n",
    "        lstm_out, self.hidden = self.lstm(x, self.hidden)\n",
    "        y = self.fc0(torch.cat([self.hidden[0][-1], self.hidden[0][-2]], 1))\n",
    "        y = self.do(y)\n",
    "        if self.activate == 'tanh':\n",
    "            y = self.fc1(F.tanh(y))\n",
    "        elif self.activate == 'relu':\n",
    "            y = self.fc1(F.relu(y))\n",
    "        tag_scores = F.log_softmax(y)\n",
    "        return tag_scores\n",
    "\n",
    "    def init_hidden(self):\n",
    "        # The axes semantics are (num_layers, minibatch_size, hidden_dim)\n",
    "        num = 2 if self.bidirectional else 1    # bidirectionalのとき2\n",
    "        h0 = torch.zeros(num, self.batch_size, self.hidden_dim).to(self.device)\n",
    "        c0 = torch.zeros(num, self.batch_size, self.hidden_dim).to(self.device)\n",
    "        return (h0, c0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(net, train_loader, epoch_num):\n",
    "\n",
    "    for epoch in range(epoch_num):\n",
    "\n",
    "        train_loss = 0.0\n",
    "        train_acc  = 0.0\n",
    "\n",
    "        # train====================\n",
    "        net.train()\n",
    "        for xx, yy in train_loader:\n",
    "            xx, yy = xx.long().to(device), yy.to(device)\n",
    "\n",
    "            net.batch_size = len(yy)\n",
    "            net.hidden = net.init_hidden()\n",
    "\n",
    "            optimizer.zero_grad()    # 勾配の初期化\n",
    "\n",
    "            output = net(xx)\n",
    "            loss   = criterion(output, yy)\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            train_acc += (output.max(1)[1] == yy).sum().item()\n",
    "\n",
    "            loss.backward(retain_graph=True)     # 逆伝播の計算\n",
    "            optimizer.step()    # 勾配の更新"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(net, test_loader, y_test):\n",
    "    net.eval()\n",
    "    y_pred = []\n",
    "    with torch.no_grad():\n",
    "        for xx, yy in test_loader:\n",
    "            xx, yy = xx.long().to(device), yy.to(device)\n",
    "\n",
    "            net.batch_size = len(yy)\n",
    "            net.hidden = net.init_hidden()\n",
    "\n",
    "            output = net(xx)\n",
    "            y_pred += output.data.max(1, keepdim=True)[1].to('cpu').numpy()[:,0].tolist()\n",
    "\n",
    "    acc = (y_pred == y_test).sum().item() / len(y_test)\n",
    "    result = precision_recall_fscore_support(y_test, y_pred, average='macro')\n",
    "    return [acc, result[0], result[1], result[2]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results of GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>embedding_dim</th>\n",
       "      <th>hidden_dim</th>\n",
       "      <th>activate_func</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>l2_regular</th>\n",
       "      <th>dropout_rate</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>300</td>\n",
       "      <td>64</td>\n",
       "      <td>100</td>\n",
       "      <td>200</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.822355</td>\n",
       "      <td>0.818589</td>\n",
       "      <td>0.818205</td>\n",
       "      <td>0.818393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>300</td>\n",
       "      <td>64</td>\n",
       "      <td>100</td>\n",
       "      <td>200</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.816367</td>\n",
       "      <td>0.812124</td>\n",
       "      <td>0.814167</td>\n",
       "      <td>0.813023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>300</td>\n",
       "      <td>64</td>\n",
       "      <td>100</td>\n",
       "      <td>200</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.814371</td>\n",
       "      <td>0.810422</td>\n",
       "      <td>0.810048</td>\n",
       "      <td>0.810231</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    epoch  batch_size  embedding_dim  hidden_dim activate_func  learning_rate  \\\n",
       "59    300          64            100         200          tanh           0.01   \n",
       "83    300          64            100         200          tanh           0.01   \n",
       "71    300          64            100         200          relu           0.01   \n",
       "\n",
       "    l2_regular  dropout_rate  accuracy  precision    recall        f1  \n",
       "59       0.001           0.0  0.822355   0.818589  0.818205  0.818393  \n",
       "83       0.001           0.5  0.816367   0.812124  0.814167  0.813023  \n",
       "71       0.001           0.0  0.814371   0.810422  0.810048  0.810231  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_df = pd.read_csv('results/gridsearch_lstm_end2end.csv').sort_values(by=['f1'], ascending=False)\n",
    "gs_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>embedding_dim</th>\n",
       "      <th>hidden_dim</th>\n",
       "      <th>activate_func</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>l2_regular</th>\n",
       "      <th>dropout_rate</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>300</td>\n",
       "      <td>64</td>\n",
       "      <td>100</td>\n",
       "      <td>200</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.822355</td>\n",
       "      <td>0.818589</td>\n",
       "      <td>0.818205</td>\n",
       "      <td>0.818393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>300</td>\n",
       "      <td>64</td>\n",
       "      <td>100</td>\n",
       "      <td>200</td>\n",
       "      <td>tanh</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.816367</td>\n",
       "      <td>0.812124</td>\n",
       "      <td>0.814167</td>\n",
       "      <td>0.813023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>300</td>\n",
       "      <td>64</td>\n",
       "      <td>100</td>\n",
       "      <td>200</td>\n",
       "      <td>relu</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.814371</td>\n",
       "      <td>0.810422</td>\n",
       "      <td>0.810048</td>\n",
       "      <td>0.810231</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    epoch  batch_size  embedding_dim  hidden_dim activate_func  learning_rate  \\\n",
       "59    300          64            100         200          tanh           0.01   \n",
       "83    300          64            100         200          tanh           0.01   \n",
       "71    300          64            100         200          relu           0.01   \n",
       "\n",
       "    l2_regular  dropout_rate  accuracy  precision    recall        f1  \n",
       "59       0.001           0.0  0.822355   0.818589  0.818205  0.818393  \n",
       "83       0.001           0.5  0.816367   0.812124  0.814167  0.813023  \n",
       "71       0.001           0.0  0.814371   0.810422  0.810048  0.810231  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_df = pd.read_csv('results/gridsearch_lstm_end2end.csv').sort_values(by=['accuracy'], ascending=False)\n",
    "gs_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case1\n",
    "the random seed of train_test_split is 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulaly size: 9394\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv('/home/b2018yniki/data/nikkei/train.txt', sep='\\t', header=None, names=['target', 'time', 'body']).drop('time', axis=1)\n",
    "test_df  = pd.read_csv('/home/b2018yniki/data/nikkei/test.txt', sep='\\t', header=None, names=['target', 'time', 'body']).drop('time', axis=1)\n",
    "total_df = pd.concat([train_df, test_df], ignore_index=True).drop_duplicates()\n",
    "\n",
    "X = total_df.body\n",
    "y = total_df.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=2019)\n",
    "del train_df, test_df, total_df, X, y\n",
    "\n",
    "vocab = []\n",
    "for text in X_train.values:\n",
    "    vocab.extend(get_tango(text))\n",
    "vocab = list(set(vocab))\n",
    "print('vocabulaly size: {}'.format(len(vocab)))\n",
    "vocab_idx = dict(zip(vocab, range(len(vocab))))\n",
    "del vocab\n",
    "\n",
    "X_train = df2input(X_train, vocab_idx)\n",
    "X_test  = df2input(X_test, vocab_idx)\n",
    "\n",
    "train_ds = MyDataset(X_train, y_train.values)\n",
    "test_ds  = MyDataset(X_test, y_test.values)\n",
    "\n",
    "del X_train, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM(\n",
      "  (emb): Embedding(9394, 100)\n",
      "  (lstm): LSTM(100, 200, batch_first=True, bidirectional=True)\n",
      "  (fc0): Linear(in_features=400, out_features=100, bias=True)\n",
      "  (fc1): Linear(in_features=100, out_features=2, bias=True)\n",
      "  (do): Dropout(p=0.0)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/b2018yniki/.pyenv/versions/3.7.1/lib/python3.7/site-packages/torch/nn/functional.py:1374: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "/home/b2018yniki/.pyenv/versions/3.7.1/lib/python3.7/site-packages/ipykernel_launcher.py:31: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8023952095808383, Precision: 0.7990276862228082, Recall: 0.7954345631573805, F1: 0.7969559148016392\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(2019)\n",
    "np.random.RandomState(2019)\n",
    "torch.manual_seed(2019)\n",
    "\n",
    "# hyperparameter\n",
    "epoch      = 300\n",
    "batch_size = 64\n",
    "vocab_size = len(vocab_idx)\n",
    "emb_dim    = 100\n",
    "hidden_dim = 200\n",
    "activate   = 'tanh'\n",
    "drop_rate  = 0.0\n",
    "lr = 0.01\n",
    "l2 = 0.001\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "test_loader  = DataLoader(test_ds, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "np.random.seed(2019)\n",
    "np.random.RandomState(2019)\n",
    "torch.manual_seed(2019)\n",
    "\n",
    "net = LSTM(batch_size, vocab_size, emb_dim, hidden_dim, drop_rate, activate, bidirectional=True, device=device).to(device)\n",
    "\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=lr, weight_decay=l2)\n",
    "\n",
    "print(net)\n",
    "\n",
    "training(net, train_loader, epoch)\n",
    "result = test(net, test_loader, y_test)\n",
    "print('Accuracy: {}, Precision: {}, Recall: {}, F1: {}'.format(result[0], result[1], result[2], result[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net.state_dict(), 'best_params/lstm_s2019.prm')\n",
    "del net, criterion, optimizer, vocab_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8023952095808383, Precision: 0.7990276862228082, Recall: 0.7954345631573805, F1: 0.7969559148016392\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/b2018yniki/.pyenv/versions/3.7.1/lib/python3.7/site-packages/torch/nn/functional.py:1374: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "/home/b2018yniki/.pyenv/versions/3.7.1/lib/python3.7/site-packages/ipykernel_launcher.py:31: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    }
   ],
   "source": [
    "net = LSTM(batch_size, vocab_size, emb_dim, hidden_dim, drop_rate, activate, bidirectional=True, device=device).to(device)\n",
    "net.load_state_dict(torch.load('best_params/lstm_s2019.prm'))\n",
    "result = test(net, test_loader, y_test)\n",
    "print('Accuracy: {}, Precision: {}, Recall: {}, F1: {}'.format(result[0], result[1], result[2], result[3]))\n",
    "\n",
    "del net, train_ds, test_ds, train_loader, test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case2\n",
    "the random seed of train_test_split is 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulaly size: 9383\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv('/home/b2018yniki/data/nikkei/train.txt', sep='\\t', header=None, names=['target', 'time', 'body']).drop('time', axis=1)\n",
    "test_df  = pd.read_csv('/home/b2018yniki/data/nikkei/test.txt', sep='\\t', header=None, names=['target', 'time', 'body']).drop('time', axis=1)\n",
    "total_df = pd.concat([train_df, test_df], ignore_index=True).drop_duplicates()\n",
    "\n",
    "X = total_df.body\n",
    "y = total_df.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=2020)\n",
    "del train_df, test_df, total_df, X, y\n",
    "\n",
    "vocab = []\n",
    "for text in X_train.values:\n",
    "    vocab.extend(get_tango(text))\n",
    "vocab = list(set(vocab))\n",
    "print('vocabulaly size: {}'.format(len(vocab)))\n",
    "vocab_idx = dict(zip(vocab, range(len(vocab))))\n",
    "del vocab\n",
    "\n",
    "X_train = df2input(X_train, vocab_idx)\n",
    "X_test  = df2input(X_test, vocab_idx)\n",
    "\n",
    "train_ds = MyDataset(X_train, y_train.values)\n",
    "test_ds  = MyDataset(X_test, y_test.values)\n",
    "\n",
    "del X_train, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM(\n",
      "  (emb): Embedding(9383, 100)\n",
      "  (lstm): LSTM(100, 200, batch_first=True, bidirectional=True)\n",
      "  (fc0): Linear(in_features=400, out_features=100, bias=True)\n",
      "  (fc1): Linear(in_features=100, out_features=2, bias=True)\n",
      "  (do): Dropout(p=0.0)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/b2018yniki/.pyenv/versions/3.7.1/lib/python3.7/site-packages/torch/nn/functional.py:1374: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "/home/b2018yniki/.pyenv/versions/3.7.1/lib/python3.7/site-packages/ipykernel_launcher.py:31: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.688622754491018, Precision: 0.7788379073756433, Recall: 0.647406004080443, F1: 0.6260049000842203\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(2019)\n",
    "np.random.RandomState(2019)\n",
    "torch.manual_seed(2019)\n",
    "\n",
    "# hyperparameter\n",
    "epoch      = 300\n",
    "batch_size = 64\n",
    "vocab_size = len(vocab_idx)\n",
    "emb_dim    = 100\n",
    "hidden_dim = 200\n",
    "activate   = 'tanh'\n",
    "drop_rate  = 0.0\n",
    "lr = 0.01\n",
    "l2 = 0.001\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "test_loader  = DataLoader(test_ds, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "np.random.seed(2019)\n",
    "np.random.RandomState(2019)\n",
    "torch.manual_seed(2019)\n",
    "\n",
    "net = LSTM(batch_size, vocab_size, emb_dim, hidden_dim, drop_rate, activate, bidirectional=True, device=device).to(device)\n",
    "\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=lr, weight_decay=l2)\n",
    "\n",
    "print(net)\n",
    "\n",
    "training(net, train_loader, epoch)\n",
    "result = test(net, test_loader, y_test)\n",
    "print('Accuracy: {}, Precision: {}, Recall: {}, F1: {}'.format(result[0], result[1], result[2], result[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/b2018yniki/.pyenv/versions/3.7.1/lib/python3.7/site-packages/ipykernel_launcher.py:31: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.688622754491018, Precision: 0.7788379073756433, Recall: 0.647406004080443, F1: 0.6260049000842203\n"
     ]
    }
   ],
   "source": [
    "torch.save(net.state_dict(), 'best_params/lstm_s2020.prm')\n",
    "del net, criterion, optimizer, vocab_idx\n",
    "\n",
    "net = LSTM(batch_size, vocab_size, emb_dim, hidden_dim, drop_rate, activate, bidirectional=True, device=device).to(device)\n",
    "net.load_state_dict(torch.load('best_params/lstm_s2020.prm'))\n",
    "result = test(net, test_loader, y_test)\n",
    "print('Accuracy: {}, Precision: {}, Recall: {}, F1: {}'.format(result[0], result[1], result[2], result[3]))\n",
    "\n",
    "del net, train_ds, test_ds, train_loader, test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case3\n",
    "the random seed of train_test_split is 1996"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulaly size: 9282\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv('/home/b2018yniki/data/nikkei/train.txt', sep='\\t', header=None, names=['target', 'time', 'body']).drop('time', axis=1)\n",
    "test_df  = pd.read_csv('/home/b2018yniki/data/nikkei/test.txt', sep='\\t', header=None, names=['target', 'time', 'body']).drop('time', axis=1)\n",
    "total_df = pd.concat([train_df, test_df], ignore_index=True).drop_duplicates()\n",
    "\n",
    "X = total_df.body\n",
    "y = total_df.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=1996)\n",
    "del train_df, test_df, total_df, X, y\n",
    "\n",
    "vocab = []\n",
    "for text in X_train.values:\n",
    "    vocab.extend(get_tango(text))\n",
    "vocab = list(set(vocab))\n",
    "print('vocabulaly size: {}'.format(len(vocab)))\n",
    "vocab_idx = dict(zip(vocab, range(len(vocab))))\n",
    "del vocab\n",
    "\n",
    "X_train = df2input(X_train, vocab_idx)\n",
    "X_test  = df2input(X_test, vocab_idx)\n",
    "\n",
    "train_ds = MyDataset(X_train, y_train.values)\n",
    "test_ds  = MyDataset(X_test, y_test.values)\n",
    "\n",
    "del X_train, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM(\n",
      "  (emb): Embedding(9282, 100)\n",
      "  (lstm): LSTM(100, 200, batch_first=True, bidirectional=True)\n",
      "  (fc0): Linear(in_features=400, out_features=100, bias=True)\n",
      "  (fc1): Linear(in_features=100, out_features=2, bias=True)\n",
      "  (do): Dropout(p=0.0)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/b2018yniki/.pyenv/versions/3.7.1/lib/python3.7/site-packages/ipykernel_launcher.py:31: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.812375249500998, Precision: 0.8084306510958246, Recall: 0.8159205610539424, F1: 0.8099147534631406\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(2019)\n",
    "np.random.RandomState(2019)\n",
    "torch.manual_seed(2019)\n",
    "\n",
    "# hyperparameter\n",
    "epoch      = 300\n",
    "batch_size = 64\n",
    "vocab_size = len(vocab_idx)\n",
    "emb_dim    = 100\n",
    "hidden_dim = 200\n",
    "activate   = 'tanh'\n",
    "drop_rate  = 0.0\n",
    "lr = 0.01\n",
    "l2 = 0.001\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "test_loader  = DataLoader(test_ds, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "np.random.seed(2019)\n",
    "np.random.RandomState(2019)\n",
    "torch.manual_seed(2019)\n",
    "\n",
    "net = LSTM(batch_size, vocab_size, emb_dim, hidden_dim, drop_rate, activate, bidirectional=True, device=device).to(device)\n",
    "\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=lr, weight_decay=l2)\n",
    "\n",
    "print(net)\n",
    "\n",
    "training(net, train_loader, epoch)\n",
    "result = test(net, test_loader, y_test)\n",
    "print('Accuracy: {}, Precision: {}, Recall: {}, F1: {}'.format(result[0], result[1], result[2], result[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/b2018yniki/.pyenv/versions/3.7.1/lib/python3.7/site-packages/ipykernel_launcher.py:31: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.812375249500998, Precision: 0.8084306510958246, Recall: 0.8159205610539424, F1: 0.8099147534631406\n"
     ]
    }
   ],
   "source": [
    "torch.save(net.state_dict(), 'best_params/lstm_s1996.prm')\n",
    "del net, criterion, optimizer\n",
    "\n",
    "net = LSTM(batch_size, vocab_size, emb_dim, hidden_dim, drop_rate, activate, bidirectional=True, device=device).to(device)\n",
    "net.load_state_dict(torch.load('best_params/lstm_s1996.prm'))\n",
    "result = test(net, test_loader, y_test)\n",
    "print('Accuracy: {}, Precision: {}, Recall: {}, F1: {}'.format(result[0], result[1], result[2], result[3]))\n",
    "\n",
    "del net, train_ds, test_ds, train_loader, test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case4\n",
    "the random seed of train_test_split is 1192"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulaly size: 9420\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv('/home/b2018yniki/data/nikkei/train.txt', sep='\\t', header=None, names=['target', 'time', 'body']).drop('time', axis=1)\n",
    "test_df  = pd.read_csv('/home/b2018yniki/data/nikkei/test.txt', sep='\\t', header=None, names=['target', 'time', 'body']).drop('time', axis=1)\n",
    "total_df = pd.concat([train_df, test_df], ignore_index=True).drop_duplicates()\n",
    "\n",
    "X = total_df.body\n",
    "y = total_df.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=1192)\n",
    "del train_df, test_df, total_df, X, y\n",
    "\n",
    "vocab = []\n",
    "for text in X_train.values:\n",
    "    vocab.extend(get_tango(text))\n",
    "vocab = list(set(vocab))\n",
    "print('vocabulaly size: {}'.format(len(vocab)))\n",
    "vocab_idx = dict(zip(vocab, range(len(vocab))))\n",
    "del vocab\n",
    "\n",
    "X_train = df2input(X_train, vocab_idx)\n",
    "X_test  = df2input(X_test, vocab_idx)\n",
    "\n",
    "train_ds = MyDataset(X_train, y_train.values)\n",
    "test_ds  = MyDataset(X_test, y_test.values)\n",
    "\n",
    "del X_train, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM(\n",
      "  (emb): Embedding(9420, 100)\n",
      "  (lstm): LSTM(100, 200, batch_first=True, bidirectional=True)\n",
      "  (fc0): Linear(in_features=400, out_features=100, bias=True)\n",
      "  (fc1): Linear(in_features=100, out_features=2, bias=True)\n",
      "  (do): Dropout(p=0.0)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/b2018yniki/.pyenv/versions/3.7.1/lib/python3.7/site-packages/ipykernel_launcher.py:31: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8083832335329342, Precision: 0.8034973034809609, Recall: 0.8034973034809609, F1: 0.8034973034809609\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(2019)\n",
    "np.random.RandomState(2019)\n",
    "torch.manual_seed(2019)\n",
    "\n",
    "# hyperparameter\n",
    "epoch      = 300\n",
    "batch_size = 64\n",
    "vocab_size = len(vocab_idx)\n",
    "emb_dim    = 100\n",
    "hidden_dim = 200\n",
    "activate   = 'tanh'\n",
    "drop_rate  = 0.0\n",
    "lr = 0.01\n",
    "l2 = 0.001\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "test_loader  = DataLoader(test_ds, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "np.random.seed(2019)\n",
    "np.random.RandomState(2019)\n",
    "torch.manual_seed(2019)\n",
    "\n",
    "net = LSTM(batch_size, vocab_size, emb_dim, hidden_dim, drop_rate, activate, bidirectional=True, device=device).to(device)\n",
    "\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=lr, weight_decay=l2)\n",
    "\n",
    "print(net)\n",
    "\n",
    "training(net, train_loader, epoch)\n",
    "result = test(net, test_loader, y_test)\n",
    "print('Accuracy: {}, Precision: {}, Recall: {}, F1: {}'.format(result[0], result[1], result[2], result[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8083832335329342, Precision: 0.8034973034809609, Recall: 0.8034973034809609, F1: 0.8034973034809609\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/b2018yniki/.pyenv/versions/3.7.1/lib/python3.7/site-packages/ipykernel_launcher.py:31: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    }
   ],
   "source": [
    "torch.save(net.state_dict(), 'best_params/lstm_s1192.prm')\n",
    "del net, criterion, optimizer, vocab_idx\n",
    "\n",
    "net = LSTM(batch_size, vocab_size, emb_dim, hidden_dim, drop_rate, activate, bidirectional=True, device=device).to(device)\n",
    "net.load_state_dict(torch.load('best_params/lstm_s1192.prm'))\n",
    "result = test(net, test_loader, y_test)\n",
    "print('Accuracy: {}, Precision: {}, Recall: {}, F1: {}'.format(result[0], result[1], result[2], result[3]))\n",
    "\n",
    "del net, train_ds, test_ds, train_loader, test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case5\n",
    "the random seed of train_test_split is 794"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulaly size: 9359\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv('/home/b2018yniki/data/nikkei/train.txt', sep='\\t', header=None, names=['target', 'time', 'body']).drop('time', axis=1)\n",
    "test_df  = pd.read_csv('/home/b2018yniki/data/nikkei/test.txt', sep='\\t', header=None, names=['target', 'time', 'body']).drop('time', axis=1)\n",
    "total_df = pd.concat([train_df, test_df], ignore_index=True).drop_duplicates()\n",
    "\n",
    "X = total_df.body\n",
    "y = total_df.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=794)\n",
    "del train_df, test_df, total_df, X, y\n",
    "\n",
    "vocab = []\n",
    "for text in X_train.values:\n",
    "    vocab.extend(get_tango(text))\n",
    "vocab = list(set(vocab))\n",
    "print('vocabulaly size: {}'.format(len(vocab)))\n",
    "vocab_idx = dict(zip(vocab, range(len(vocab))))\n",
    "del vocab\n",
    "\n",
    "X_train = df2input(X_train, vocab_idx)\n",
    "X_test  = df2input(X_test, vocab_idx)\n",
    "\n",
    "train_ds = MyDataset(X_train, y_train.values)\n",
    "test_ds  = MyDataset(X_test, y_test.values)\n",
    "\n",
    "del X_train, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM(\n",
      "  (emb): Embedding(9359, 100)\n",
      "  (lstm): LSTM(100, 200, batch_first=True, bidirectional=True)\n",
      "  (fc0): Linear(in_features=400, out_features=100, bias=True)\n",
      "  (fc1): Linear(in_features=100, out_features=2, bias=True)\n",
      "  (do): Dropout(p=0.0)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/b2018yniki/.pyenv/versions/3.7.1/lib/python3.7/site-packages/ipykernel_launcher.py:31: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8083832335329342, Precision: 0.8044232922732363, Recall: 0.8118158877892115, F1: 0.8058703865155479\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(2019)\n",
    "np.random.RandomState(2019)\n",
    "torch.manual_seed(2019)\n",
    "\n",
    "# hyperparameter\n",
    "epoch      = 300\n",
    "batch_size = 64\n",
    "vocab_size = len(vocab_idx)\n",
    "emb_dim    = 100\n",
    "hidden_dim = 200\n",
    "activate   = 'tanh'\n",
    "drop_rate  = 0.0\n",
    "lr = 0.01\n",
    "l2 = 0.001\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "test_loader  = DataLoader(test_ds, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "np.random.seed(2019)\n",
    "np.random.RandomState(2019)\n",
    "torch.manual_seed(2019)\n",
    "\n",
    "net = LSTM(batch_size, vocab_size, emb_dim, hidden_dim, drop_rate, activate, bidirectional=True, device=device).to(device)\n",
    "\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=lr, weight_decay=l2)\n",
    "\n",
    "print(net)\n",
    "\n",
    "training(net, train_loader, epoch)\n",
    "result = test(net, test_loader, y_test)\n",
    "print('Accuracy: {}, Precision: {}, Recall: {}, F1: {}'.format(result[0], result[1], result[2], result[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/b2018yniki/.pyenv/versions/3.7.1/lib/python3.7/site-packages/ipykernel_launcher.py:31: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8083832335329342, Precision: 0.8044232922732363, Recall: 0.8118158877892115, F1: 0.8058703865155479\n"
     ]
    }
   ],
   "source": [
    "torch.save(net.state_dict(), 'best_params/lstm_s794.prm')\n",
    "del net, criterion, optimizer, vocab_idx\n",
    "\n",
    "net = LSTM(batch_size, vocab_size, emb_dim, hidden_dim, drop_rate, activate, bidirectional=True, device=device).to(device)\n",
    "net.load_state_dict(torch.load('best_params/lstm_s794.prm'))\n",
    "result = test(net, test_loader, y_test)\n",
    "print('Accuracy: {}, Precision: {}, Recall: {}, F1: {}'.format(result[0], result[1], result[2], result[3]))\n",
    "\n",
    "del net, train_ds, test_ds, train_loader, test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case6\n",
    "the random seed of train_test_split is 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulaly size: 9413\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv('/home/b2018yniki/data/nikkei/train.txt', sep='\\t', header=None, names=['target', 'time', 'body']).drop('time', axis=1)\n",
    "test_df  = pd.read_csv('/home/b2018yniki/data/nikkei/test.txt', sep='\\t', header=None, names=['target', 'time', 'body']).drop('time', axis=1)\n",
    "total_df = pd.concat([train_df, test_df], ignore_index=True).drop_duplicates()\n",
    "\n",
    "X = total_df.body\n",
    "y = total_df.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=2000)\n",
    "del train_df, test_df, total_df, X, y\n",
    "\n",
    "vocab = []\n",
    "for text in X_train.values:\n",
    "    vocab.extend(get_tango(text))\n",
    "vocab = list(set(vocab))\n",
    "print('vocabulaly size: {}'.format(len(vocab)))\n",
    "vocab_idx = dict(zip(vocab, range(len(vocab))))\n",
    "del vocab\n",
    "\n",
    "X_train = df2input(X_train, vocab_idx)\n",
    "X_test  = df2input(X_test, vocab_idx)\n",
    "\n",
    "train_ds = MyDataset(X_train, y_train.values)\n",
    "test_ds  = MyDataset(X_test, y_test.values)\n",
    "\n",
    "del X_train, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM(\n",
      "  (emb): Embedding(9413, 100)\n",
      "  (lstm): LSTM(100, 200, batch_first=True, bidirectional=True)\n",
      "  (fc0): Linear(in_features=400, out_features=100, bias=True)\n",
      "  (fc1): Linear(in_features=100, out_features=2, bias=True)\n",
      "  (do): Dropout(p=0.0)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/b2018yniki/.pyenv/versions/3.7.1/lib/python3.7/site-packages/ipykernel_launcher.py:31: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8043912175648703, Precision: 0.801758476634606, Recall: 0.801163542340013, F1: 0.8014477515367195\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(2019)\n",
    "np.random.RandomState(2019)\n",
    "torch.manual_seed(2019)\n",
    "\n",
    "# hyperparameter\n",
    "epoch      = 300\n",
    "batch_size = 64\n",
    "vocab_size = len(vocab_idx)\n",
    "emb_dim    = 100\n",
    "hidden_dim = 200\n",
    "activate   = 'tanh'\n",
    "drop_rate  = 0.0\n",
    "lr = 0.01\n",
    "l2 = 0.001\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "test_loader  = DataLoader(test_ds, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "np.random.seed(2019)\n",
    "np.random.RandomState(2019)\n",
    "torch.manual_seed(2019)\n",
    "\n",
    "net = LSTM(batch_size, vocab_size, emb_dim, hidden_dim, drop_rate, activate, bidirectional=True, device=device).to(device)\n",
    "\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=lr, weight_decay=l2)\n",
    "\n",
    "print(net)\n",
    "\n",
    "training(net, train_loader, epoch)\n",
    "result = test(net, test_loader, y_test)\n",
    "print('Accuracy: {}, Precision: {}, Recall: {}, F1: {}'.format(result[0], result[1], result[2], result[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/b2018yniki/.pyenv/versions/3.7.1/lib/python3.7/site-packages/ipykernel_launcher.py:31: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8043912175648703, Precision: 0.801758476634606, Recall: 0.801163542340013, F1: 0.8014477515367195\n"
     ]
    }
   ],
   "source": [
    "torch.save(net.state_dict(), 'best_params/lstm_s2000.prm')\n",
    "del net, criterion, optimizer, vocab_idx\n",
    "\n",
    "net = LSTM(batch_size, vocab_size, emb_dim, hidden_dim, drop_rate, activate, bidirectional=True, device=device).to(device)\n",
    "net.load_state_dict(torch.load('best_params/lstm_s2000.prm'))\n",
    "result = test(net, test_loader, y_test)\n",
    "print('Accuracy: {}, Precision: {}, Recall: {}, F1: {}'.format(result[0], result[1], result[2], result[3]))\n",
    "\n",
    "del net, train_ds, test_ds, train_loader, test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case7\n",
    "the random seed of train_test_split is 1945"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulaly size: 9408\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv('/home/b2018yniki/data/nikkei/train.txt', sep='\\t', header=None, names=['target', 'time', 'body']).drop('time', axis=1)\n",
    "test_df  = pd.read_csv('/home/b2018yniki/data/nikkei/test.txt', sep='\\t', header=None, names=['target', 'time', 'body']).drop('time', axis=1)\n",
    "total_df = pd.concat([train_df, test_df], ignore_index=True).drop_duplicates()\n",
    "\n",
    "X = total_df.body\n",
    "y = total_df.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=1945)\n",
    "del train_df, test_df, total_df, X, y\n",
    "\n",
    "vocab = []\n",
    "for text in X_train.values:\n",
    "    vocab.extend(get_tango(text))\n",
    "vocab = list(set(vocab))\n",
    "print('vocabulaly size: {}'.format(len(vocab)))\n",
    "vocab_idx = dict(zip(vocab, range(len(vocab))))\n",
    "del vocab\n",
    "\n",
    "X_train = df2input(X_train, vocab_idx)\n",
    "X_test  = df2input(X_test, vocab_idx)\n",
    "\n",
    "train_ds = MyDataset(X_train, y_train.values)\n",
    "test_ds  = MyDataset(X_test, y_test.values)\n",
    "\n",
    "del X_train, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM(\n",
      "  (emb): Embedding(9408, 100)\n",
      "  (lstm): LSTM(100, 200, batch_first=True, bidirectional=True)\n",
      "  (fc0): Linear(in_features=400, out_features=100, bias=True)\n",
      "  (fc1): Linear(in_features=100, out_features=2, bias=True)\n",
      "  (do): Dropout(p=0.0)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/b2018yniki/.pyenv/versions/3.7.1/lib/python3.7/site-packages/torch/nn/functional.py:1374: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "/home/b2018yniki/.pyenv/versions/3.7.1/lib/python3.7/site-packages/ipykernel_launcher.py:31: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8043912175648703, Precision: 0.8148032426954906, Recall: 0.7981842905135179, F1: 0.7999070813771518\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(2019)\n",
    "np.random.RandomState(2019)\n",
    "torch.manual_seed(2019)\n",
    "\n",
    "# hyperparameter\n",
    "epoch      = 300\n",
    "batch_size = 64\n",
    "vocab_size = len(vocab_idx)\n",
    "emb_dim    = 100\n",
    "hidden_dim = 200\n",
    "activate   = 'tanh'\n",
    "drop_rate  = 0.0\n",
    "lr = 0.01\n",
    "l2 = 0.001\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "test_loader  = DataLoader(test_ds, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "np.random.seed(2019)\n",
    "np.random.RandomState(2019)\n",
    "torch.manual_seed(2019)\n",
    "\n",
    "net = LSTM(batch_size, vocab_size, emb_dim, hidden_dim, drop_rate, activate, bidirectional=True, device=device).to(device)\n",
    "\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=lr, weight_decay=l2)\n",
    "\n",
    "print(net)\n",
    "\n",
    "training(net, train_loader, epoch)\n",
    "result = test(net, test_loader, y_test)\n",
    "print('Accuracy: {}, Precision: {}, Recall: {}, F1: {}'.format(result[0], result[1], result[2], result[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8043912175648703, Precision: 0.8148032426954906, Recall: 0.7981842905135179, F1: 0.7999070813771518\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/b2018yniki/.pyenv/versions/3.7.1/lib/python3.7/site-packages/ipykernel_launcher.py:31: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    }
   ],
   "source": [
    "torch.save(net.state_dict(), 'best_params/lstm_s1945.prm')\n",
    "del net, criterion, optimizer, vocab_idx\n",
    "\n",
    "net = LSTM(batch_size, vocab_size, emb_dim, hidden_dim, drop_rate, activate, bidirectional=True, device=device).to(device)\n",
    "net.load_state_dict(torch.load('best_params/lstm_s1945.prm'))\n",
    "result = test(net, test_loader, y_test)\n",
    "print('Accuracy: {}, Precision: {}, Recall: {}, F1: {}'.format(result[0], result[1], result[2], result[3]))\n",
    "\n",
    "del net, train_ds, test_ds, train_loader, test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case8\n",
    "the random seed of train_test_split is 5748"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulaly size: 9360\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv('/home/b2018yniki/data/nikkei/train.txt', sep='\\t', header=None, names=['target', 'time', 'body']).drop('time', axis=1)\n",
    "test_df  = pd.read_csv('/home/b2018yniki/data/nikkei/test.txt', sep='\\t', header=None, names=['target', 'time', 'body']).drop('time', axis=1)\n",
    "total_df = pd.concat([train_df, test_df], ignore_index=True).drop_duplicates()\n",
    "\n",
    "X = total_df.body\n",
    "y = total_df.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=5748)\n",
    "del train_df, test_df, total_df, X, y\n",
    "\n",
    "vocab = []\n",
    "for text in X_train.values:\n",
    "    vocab.extend(get_tango(text))\n",
    "vocab = list(set(vocab))\n",
    "print('vocabulaly size: {}'.format(len(vocab)))\n",
    "vocab_idx = dict(zip(vocab, range(len(vocab))))\n",
    "del vocab\n",
    "\n",
    "X_train = df2input(X_train, vocab_idx)\n",
    "X_test  = df2input(X_test, vocab_idx)\n",
    "\n",
    "train_ds = MyDataset(X_train, y_train.values)\n",
    "test_ds  = MyDataset(X_test, y_test.values)\n",
    "\n",
    "del X_train, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM(\n",
      "  (emb): Embedding(9360, 100)\n",
      "  (lstm): LSTM(100, 200, batch_first=True, bidirectional=True)\n",
      "  (fc0): Linear(in_features=400, out_features=100, bias=True)\n",
      "  (fc1): Linear(in_features=100, out_features=2, bias=True)\n",
      "  (do): Dropout(p=0.0)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/b2018yniki/.pyenv/versions/3.7.1/lib/python3.7/site-packages/torch/nn/functional.py:1374: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "/home/b2018yniki/.pyenv/versions/3.7.1/lib/python3.7/site-packages/ipykernel_launcher.py:31: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.782435129740519, Precision: 0.8004494198398431, Recall: 0.798314078016486, F1: 0.7824039208654594\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(2019)\n",
    "np.random.RandomState(2019)\n",
    "torch.manual_seed(2019)\n",
    "\n",
    "# hyperparameter\n",
    "epoch      = 300\n",
    "batch_size = 64\n",
    "vocab_size = len(vocab_idx)\n",
    "emb_dim    = 100\n",
    "hidden_dim = 200\n",
    "activate   = 'tanh'\n",
    "drop_rate  = 0.0\n",
    "lr = 0.01\n",
    "l2 = 0.001\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "test_loader  = DataLoader(test_ds, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "np.random.seed(2019)\n",
    "np.random.RandomState(2019)\n",
    "torch.manual_seed(2019)\n",
    "\n",
    "net = LSTM(batch_size, vocab_size, emb_dim, hidden_dim, drop_rate, activate, bidirectional=True, device=device).to(device)\n",
    "\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=lr, weight_decay=l2)\n",
    "\n",
    "print(net)\n",
    "\n",
    "training(net, train_loader, epoch)\n",
    "result = test(net, test_loader, y_test)\n",
    "print('Accuracy: {}, Precision: {}, Recall: {}, F1: {}'.format(result[0], result[1], result[2], result[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.782435129740519, Precision: 0.8004494198398431, Recall: 0.798314078016486, F1: 0.7824039208654594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/b2018yniki/.pyenv/versions/3.7.1/lib/python3.7/site-packages/ipykernel_launcher.py:31: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    }
   ],
   "source": [
    "torch.save(net.state_dict(), 'best_params/lstm_s5748.prm')\n",
    "del net, criterion, optimizer, vocab_idx\n",
    "\n",
    "net = LSTM(batch_size, vocab_size, emb_dim, hidden_dim, drop_rate, activate, bidirectional=True, device=device).to(device)\n",
    "net.load_state_dict(torch.load('best_params/lstm_s5748.prm'))\n",
    "result = test(net, test_loader, y_test)\n",
    "print('Accuracy: {}, Precision: {}, Recall: {}, F1: {}'.format(result[0], result[1], result[2], result[3]))\n",
    "\n",
    "del net, train_ds, test_ds, train_loader, test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case9\n",
    "the random seed of train_test_split is 7248"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulaly size: 9450\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv('/home/b2018yniki/data/nikkei/train.txt', sep='\\t', header=None, names=['target', 'time', 'body']).drop('time', axis=1)\n",
    "test_df  = pd.read_csv('/home/b2018yniki/data/nikkei/test.txt', sep='\\t', header=None, names=['target', 'time', 'body']).drop('time', axis=1)\n",
    "total_df = pd.concat([train_df, test_df], ignore_index=True).drop_duplicates()\n",
    "\n",
    "X = total_df.body\n",
    "y = total_df.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=7248)\n",
    "del train_df, test_df, total_df, X, y\n",
    "\n",
    "vocab = []\n",
    "for text in X_train.values:\n",
    "    vocab.extend(get_tango(text))\n",
    "vocab = list(set(vocab))\n",
    "print('vocabulaly size: {}'.format(len(vocab)))\n",
    "vocab_idx = dict(zip(vocab, range(len(vocab))))\n",
    "del vocab\n",
    "\n",
    "X_train = df2input(X_train, vocab_idx)\n",
    "X_test  = df2input(X_test, vocab_idx)\n",
    "\n",
    "train_ds = MyDataset(X_train, y_train.values)\n",
    "test_ds  = MyDataset(X_test, y_test.values)\n",
    "\n",
    "del X_train, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM(\n",
      "  (emb): Embedding(9450, 100)\n",
      "  (lstm): LSTM(100, 200, batch_first=True, bidirectional=True)\n",
      "  (fc0): Linear(in_features=400, out_features=100, bias=True)\n",
      "  (fc1): Linear(in_features=100, out_features=2, bias=True)\n",
      "  (do): Dropout(p=0.0)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/b2018yniki/.pyenv/versions/3.7.1/lib/python3.7/site-packages/ipykernel_launcher.py:31: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8283433133732535, Precision: 0.8213611969425922, Recall: 0.8266522299732204, F1: 0.8234990496165695\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(2019)\n",
    "np.random.RandomState(2019)\n",
    "torch.manual_seed(2019)\n",
    "\n",
    "# hyperparameter\n",
    "epoch      = 300\n",
    "batch_size = 64\n",
    "vocab_size = len(vocab_idx)\n",
    "emb_dim    = 100\n",
    "hidden_dim = 200\n",
    "activate   = 'tanh'\n",
    "drop_rate  = 0.0\n",
    "lr = 0.01\n",
    "l2 = 0.001\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "test_loader  = DataLoader(test_ds, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "np.random.seed(2019)\n",
    "np.random.RandomState(2019)\n",
    "torch.manual_seed(2019)\n",
    "\n",
    "net = LSTM(batch_size, vocab_size, emb_dim, hidden_dim, drop_rate, activate, bidirectional=True, device=device).to(device)\n",
    "\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=lr, weight_decay=l2)\n",
    "\n",
    "print(net)\n",
    "\n",
    "training(net, train_loader, epoch)\n",
    "result = test(net, test_loader, y_test)\n",
    "print('Accuracy: {}, Precision: {}, Recall: {}, F1: {}'.format(result[0], result[1], result[2], result[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8283433133732535, Precision: 0.8213611969425922, Recall: 0.8266522299732204, F1: 0.8234990496165695\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/b2018yniki/.pyenv/versions/3.7.1/lib/python3.7/site-packages/ipykernel_launcher.py:31: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    }
   ],
   "source": [
    "torch.save(net.state_dict(), 'best_params/lstm_s7248.prm')\n",
    "del net, criterion, optimizer, vocab_idx\n",
    "\n",
    "net = LSTM(batch_size, vocab_size, emb_dim, hidden_dim, drop_rate, activate, bidirectional=True, device=device).to(device)\n",
    "net.load_state_dict(torch.load('best_params/lstm_s7248.prm'))\n",
    "result = test(net, test_loader, y_test)\n",
    "print('Accuracy: {}, Precision: {}, Recall: {}, F1: {}'.format(result[0], result[1], result[2], result[3]))\n",
    "\n",
    "del net, train_ds, test_ds, train_loader, test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case10\n",
    "the random seed of train_test_split is 8787"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulaly size: 9466\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv('/home/b2018yniki/data/nikkei/train.txt', sep='\\t', header=None, names=['target', 'time', 'body']).drop('time', axis=1)\n",
    "test_df  = pd.read_csv('/home/b2018yniki/data/nikkei/test.txt', sep='\\t', header=None, names=['target', 'time', 'body']).drop('time', axis=1)\n",
    "total_df = pd.concat([train_df, test_df], ignore_index=True).drop_duplicates()\n",
    "\n",
    "X = total_df.body\n",
    "y = total_df.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=8787)\n",
    "del train_df, test_df, total_df, X, y\n",
    "\n",
    "vocab = []\n",
    "for text in X_train.values:\n",
    "    vocab.extend(get_tango(text))\n",
    "vocab = list(set(vocab))\n",
    "print('vocabulaly size: {}'.format(len(vocab)))\n",
    "vocab_idx = dict(zip(vocab, range(len(vocab))))\n",
    "del vocab\n",
    "\n",
    "X_train = df2input(X_train, vocab_idx)\n",
    "X_test  = df2input(X_test, vocab_idx)\n",
    "\n",
    "train_ds = MyDataset(X_train, y_train.values)\n",
    "test_ds  = MyDataset(X_test, y_test.values)\n",
    "\n",
    "del X_train, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM(\n",
      "  (emb): Embedding(9466, 100)\n",
      "  (lstm): LSTM(100, 200, batch_first=True, bidirectional=True)\n",
      "  (fc0): Linear(in_features=400, out_features=100, bias=True)\n",
      "  (fc1): Linear(in_features=100, out_features=2, bias=True)\n",
      "  (do): Dropout(p=0.0)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/b2018yniki/.pyenv/versions/3.7.1/lib/python3.7/site-packages/torch/nn/functional.py:1374: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "/home/b2018yniki/.pyenv/versions/3.7.1/lib/python3.7/site-packages/ipykernel_launcher.py:31: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8163672654690619, Precision: 0.8098249983614079, Recall: 0.8116018457481873, F1: 0.8106575963718821\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(2019)\n",
    "np.random.RandomState(2019)\n",
    "torch.manual_seed(2019)\n",
    "\n",
    "# hyperparameter\n",
    "epoch      = 300\n",
    "batch_size = 64\n",
    "vocab_size = len(vocab_idx)\n",
    "emb_dim    = 100\n",
    "hidden_dim = 200\n",
    "activate   = 'tanh'\n",
    "drop_rate  = 0.0\n",
    "lr = 0.01\n",
    "l2 = 0.001\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "test_loader  = DataLoader(test_ds, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "np.random.seed(2019)\n",
    "np.random.RandomState(2019)\n",
    "torch.manual_seed(2019)\n",
    "\n",
    "net = LSTM(batch_size, vocab_size, emb_dim, hidden_dim, drop_rate, activate, bidirectional=True, device=device).to(device)\n",
    "\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=lr, weight_decay=l2)\n",
    "\n",
    "print(net)\n",
    "\n",
    "training(net, train_loader, epoch)\n",
    "result = test(net, test_loader, y_test)\n",
    "print('Accuracy: {}, Precision: {}, Recall: {}, F1: {}'.format(result[0], result[1], result[2], result[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8163672654690619, Precision: 0.8098249983614079, Recall: 0.8116018457481873, F1: 0.8106575963718821\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/b2018yniki/.pyenv/versions/3.7.1/lib/python3.7/site-packages/ipykernel_launcher.py:31: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    }
   ],
   "source": [
    "torch.save(net.state_dict(), 'best_params/lstm_s8787.prm')\n",
    "del net, criterion, optimizer, vocab_idx\n",
    "\n",
    "net = LSTM(batch_size, vocab_size, emb_dim, hidden_dim, drop_rate, activate, bidirectional=True, device=device).to(device)\n",
    "net.load_state_dict(torch.load('best_params/lstm_s8787.prm'))\n",
    "result = test(net, test_loader, y_test)\n",
    "print('Accuracy: {}, Precision: {}, Recall: {}, F1: {}'.format(result[0], result[1], result[2], result[3]))\n",
    "\n",
    "del net, train_ds, test_ds, train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
